{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/embedding/v-xingwuchen/miniconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 2.66G/2.66G [00:39<00:00, 68.2MB/s]\n",
      "Downloading data: 100%|██████████| 26.9M/26.9M [00:00<00:00, 42.6MB/s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:39<00:00, 19.87s/it]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1223.72it/s]\n",
      "Generating train split: 21755681 examples [00:34, 632907.12 examples/s]\n",
      "Generating validation split: 218380 examples [00:00, 694725.73 examples/s]\n",
      "Saving the dataset (6/6 shards): 100%|██████████| 21755681/21755681 [00:34<00:00, 634174.45 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 218380/218380 [00:00<00:00, 598209.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStoriesInstruct\")\n",
    "\n",
    "dataset.save_to_disk(\"/embedding/v-xingwuchen/ts_data/TinyStories/dataset/TinyStoriesInstruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 946/946 [00:00<00:00, 5.17MB/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|██████████| 249M/249M [00:03<00:00, 63.5MB/s]\n",
      "Downloading data: 100%|██████████| 248M/248M [00:03<00:00, 71.1MB/s]\n",
      "Downloading data: 100%|██████████| 246M/246M [00:03<00:00, 73.4MB/s]\n",
      "Downloading data: 100%|██████████| 248M/248M [00:03<00:00, 67.2MB/s]\n",
      "Downloading data: 100%|██████████| 9.99M/9.99M [00:00<00:00, 56.1MB/s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:14<00:00,  7.33s/it]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 816.49it/s]\n",
      "Generating train split: 2119719 examples [00:05, 372021.96 examples/s]\n",
      "Generating validation split: 21990 examples [00:00, 382633.86 examples/s]\n",
      "Saving the dataset (4/4 shards): 100%|██████████| 2119719/2119719 [00:21<00:00, 99837.58 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 21990/21990 [00:00<00:00, 100107.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "dataset.save_to_disk(\"/embedding/v-xingwuchen/ts_data/TinyStories/dataset/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/embedding/v-xingwuchen/miniconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Config,GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"/embedding/v-xingwuchen/ts_data/TinyStories/dataset/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n"
     ]
    }
   ],
   "source": [
    "for key in dataset[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {dataset['train'][0][key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=50257,  # Vocabulary size of the GPT-2 model\n",
    "    n_embd=64,  # Hidden size of the transformer embeddings\n",
    "    n_layer=8,  # Number of transformer layers\n",
    "    n_head=16,  # Number of attention heads\n",
    "    n_positions=2048,  # Maximum sequence length\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 64)\n",
       "    (wpe): Embedding(2048, 64)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x GPT2Block(\n",
       "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=64, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 3953/264965 [07:19<8:26:25,  8.59it/s] "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# %%\n",
    "\n",
    "train_loader = DataLoader(dataset['train'], batch_size=8, shuffle=True)\n",
    "\n",
    "# %%\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch in tqdm(train_loader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        tokenized = tokenizer(batch['text'], padding=True, return_tensors='pt',max_length = 512,truncation = True)['input_ids'].to(device)\n",
    "        logits = model(tokenized)[\"logits\"]\n",
    "        # print(logits.size())\n",
    "        # flatten out seq dim\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), tokenized.view(-1))\n",
    "        # tqdm.write(f\"Loss: {loss.item()}\")\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        del tokenized,logits,loss\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41541, 1556, 292, 5979, 2013, 13]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.encode(\"Mi estas Julien.\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
